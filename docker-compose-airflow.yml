version: '3.8'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.9.2
  tmpfs:
    - /tmp:size=2G
  shm_size: 2gb
  environment:
    &airflow-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    AIRFLOW__CORE__PLUGINS_FOLDER: /opt/airflow/plugins
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
    AIRFLOW__SCHEDULER__PARSING_PROCESSES: 2
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    AIRFLOW__CELERY__WORKER_CONCURRENCY: 1
    AIRFLOW__CELERY__TASK_ALWAYS_EAGER: 'false'
    AIRFLOW__TRIGGERER__DEFAULT_CAPACITY: 1000
    AIRFLOW__TRIGGERER__AUDITING_INTERVAL: 30.0
    AIRFLOW__CORE__ASYNC_TASK_EXECUTOR: 'airflow.executors.celery_executor.CeleryExecutor'
    AWS_ENDPOINT_URL: http://minio:9000
    AWS_ACCESS_KEY_ID: minioadmin
    AWS_SECRET_ACCESS_KEY: minioadmin
    JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
    CELERY_BROKER_URL: redis://redis:6379/0
    CELERY_RESULT_BACKEND: redis://redis:6379/0
    AIRFLOW__CELERY__BROKER_CONNECTION_RETRY_ON_STARTUP: 'true'
    SPARK_MASTER_URL: spark://spark-master:7077
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./jars:/opt/airflow/jars
    - /home/always/FSDS-Lab/Project-2:/home/always/FSDS-Lab/Project-2
    - /home/always/miniconda3/envs/dl:/opt/conda/envs/dl
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    - postgres
    - redis

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - fraud-detection-network

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    command: redis-server --appendonly yes
    networks:
      - fraud-detection-network

  airflow-scheduler:
    <<: *airflow-common
    user: "0:0"
    command: |
      bash -c "
        echo 'ðŸ” Ensuring Java/procps for Scheduler...'
        if [ ! -x /usr/bin/java ]; then
          apt-get update -qq && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends openjdk-17-jre-headless procps
        fi
        echo 'ðŸ”§ Installing python deps (pyspark, pandas, numpy, pyarrow, minio) ...'
        pip install --no-cache-dir pyspark==3.5.1 pandas==2.0.3 numpy==1.24.3 pyarrow==12.0.1 minio==7.1.15 | cat
        java -version
        echo 'ðŸš€ Starting Airflow Scheduler...'
        airflow scheduler
      "
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    networks:
      - fraud-detection-network

  airflow-worker:
    <<: *airflow-common
    user: "0:0"
    command: |
      bash -c "
        echo 'ðŸ” Ensuring Java/procps for Worker...'
        if [ ! -x /usr/bin/java ]; then
          apt-get update -qq && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends openjdk-17-jre-headless procps
        fi
        echo 'ðŸ”§ Installing python deps (pyspark, pandas, numpy, pyarrow, minio) ...'
        pip install --no-cache-dir pyspark==3.5.1 pandas==2.0.3 numpy==1.24.3 pyarrow==12.0.1 minio==7.1.15 | cat

        java -version
        sleep 10
        echo 'ðŸš€ Starting Airflow Worker...'
        airflow celery worker --concurrency 1
      "
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          memory: 4G
    networks:
      - fraud-detection-network

  airflow-init:
    <<: *airflow-common
    command:
      - bash
      - -c
      - |
        echo 'ðŸ” Setting up Java and Python packages for Init...'
        if [ ! -f /usr/bin/java ]; then
          echo 'ðŸ“¥ Installing Java in init container...'
          apt-get update -qq
          apt-get install -y --no-install-recommends openjdk-17-jdk
          echo 'âœ… Java installed in init'
        fi
        
        echo 'ðŸš€ Starting Airflow Init...'
        airflow db upgrade && \
        airflow users create \
          --role Admin \
          --username admin \
          --email admin@example.com \
          --firstname Admin \
          --lastname User \
          --password admin
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - fraud-detection-network

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    command:
      - bash
      - -c
      - |
        echo 'ðŸ” Setting up Java for CLI...'
        if [ ! -f /usr/bin/java ]; then
          apt-get update -qq
          apt-get install -y --no-install-recommends openjdk-17-jdk
        fi
        airflow
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - fraud-detection-network

  # spark-master:
  #   image: bitnami/spark:3.5
  #   container_name: spark-master
  #   environment:
  #     - SPARK_MODE=master
  #     - AWS_ACCESS_KEY_ID=minioadmin
  #     - AWS_SECRET_ACCESS_KEY=minioadmin
  #     - AWS_ENDPOINT_URL=http://minio:9000
  #     - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/extra-jars/*
  #   ports:
  #     - "7077:7077"
  #     - "8080:8080"
  #   volumes:
  #     - ./jars:/opt/bitnami/spark/extra-jars
  #   networks:
  #     - fraud-detection-network

  # spark-worker-1:
  #   image: bitnami/spark:3.5
  #   container_name: spark-worker-1
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - AWS_ACCESS_KEY_ID=minioadmin
  #     - AWS_SECRET_ACCESS_KEY=minioadmin
  #     - AWS_ENDPOINT_URL=http://minio:9000
  #     - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/extra-jars/*
  #   depends_on:
  #     - spark-master
  #   volumes:
  #     - ./jars:/opt/bitnami/spark/extra-jars
  #   networks:
  #     - fraud-detection-network

volumes:
  postgres-db-volume:
  airflow-logs:
  airflow-plugins:
networks:
  fraud-detection-network:
    external: true
    name: fraud-detection-project_fraud-detection-network